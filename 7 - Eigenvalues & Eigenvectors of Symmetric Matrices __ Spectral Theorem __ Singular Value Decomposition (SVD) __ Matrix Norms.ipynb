{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part revolves all around eigenvalues, eigenvectors, and corresponding matrix decompositions.\n",
    "\n",
    "Note: As in the last homework, the different tasks are only minimally interdependent, so it should be possible to solve most other tasks if you get stuck on one of them.\n",
    "\n",
    "Let's start with initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                # basic arrays, vectors, matrices\n",
    "import scipy as sp                # matrix linear algebra \n",
    "\n",
    "import matplotlib                 # plotting\n",
    "import matplotlib.pyplot as plt   # plotting\n",
    "from matplotlib.image import imread\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"<style>.output_png { display: table-cell; text-align: center; vertical-align: middle; }</style>\"\"\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "### Singular Value Decomposition (SVD)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed in the course, the singular value decomposition (SVD) can be used to compute the principal component analysis (PCA), which identifies the most important axes in a data set.\n",
    "\n",
    "In this homework, we will apply this to analyze a data set comprised of photos of faces. The data considered here is a standard data set for machine learning applications, the [Extended Yale Database (Set B)](http://vision.ucsd.edu/~leekc/ExtYaleDatabase/ExtYaleB.html). \n",
    "\n",
    "For this homework, since the point is not to exercise data loading and conversion, the data set has been converted to a format that can be directly read using NumPy functions. Let's load the data and take a first look.\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "    \n",
    "**Note:** Please download the file `faces.npz` from OLAT, and put it in the same directory as this notebook.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces = np.load(\"faces.npz\")['faces']\n",
    "print( \"data set shape:\", faces.shape )\n",
    "print( \"data set type: \", faces.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading, the `faces` array contains 1071 images of size 64x64 pixels, with integer values in the range [0, 255].\n",
    "\n",
    "To obtain a first inspection of the data, we set some default parameters for matplotlib (so we obtain nicer images), and define a convenience function `plot_face` that we will use to not have to worry about the details of plotting faces from here on out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set some matplotlib parameters for better figures\n",
    "matplotlib.rcParams[\"image.cmap\"] = 'Greys_r'\n",
    "matplotlib.rcParams[\"figure.autolayout\"] = True\n",
    "\n",
    "# define convenience function for face visualization\n",
    "def plot_face(ax, face):\n",
    "    \"\"\"convenience function for visualizing a 64x64 face vector\"\"\"\n",
    "    ax.imshow(face.reshape(64,64))\n",
    "    ax.axis('off')\n",
    "    return ax\n",
    "\n",
    "# plot the first 24 faces\n",
    "fig, axes = plt.subplots(3, 8, figsize=(16,6))\n",
    "\n",
    "for ax, face in zip(axes.flatten(), faces):\n",
    "    plot_face(ax, face)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These faces look all quite different. PCA will allow us to tell if and how the faces are correlated, and what the corresponding dimensions are.\n",
    "\n",
    "First, we prepare the data matrix $X$ by turning the 64x64 images into vectors of length 4096 (by just concatenating the rows of each image). Second, we compute the mean image $\\bar{X}$ (`Xmean`) and subtract it from all images such that we obtain mean-free data in `Xmf`\n",
    "\n",
    "Let's also look at the mean face:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn images into vectors\n",
    "X = np.reshape(faces, (1071, 4096))\n",
    "print(X.shape)\n",
    "\n",
    "# compute mean image\n",
    "Xmean = np.average(X, axis=0)\n",
    "\n",
    "\n",
    "# compute mean free data\n",
    "Xmf = X - Xmean\n",
    "\n",
    "# visualize mean face\n",
    "plot_face(plt.gca(), Xmean);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do the images look like with removed mean? Simply, just plot them again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 8, figsize=(16,6))\n",
    "\n",
    "for ax, face in zip(axes.flatten(), Xmf):\n",
    "    plot_face(ax, face)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not dramatically different, but can see that distinctive features (image regions which are far from the mean) stick out a little more than before mean removal.\n",
    "\n",
    "As discussed, the principal components can be obtained from the SVD of the mean-free data matrix. Since $X$ is a $(1071\\times 4096)$-matrix, we compute the decomposition\n",
    "\n",
    "$$\n",
    "X - (\\bar X, \\dots, \\bar X)^T  = U\\,\\Sigma\\,V^T\n",
    "$$\n",
    "\n",
    "where $\\Sigma$ is again a $(1071\\times 4096)$-matrix that contains only the singular values $\\sigma_i$ on the diagonal and zeros otherwise. $V = (\\mathbf{v}_1,\\dots,\\mathbf{v}_{4096})$ is a $(4096\\times 4096)$-matrix that contains as its columns a basis of the space of all images (4096 basis vectors of size 4096 each, called the right singular vectors).\n",
    "\n",
    "[`numpy.linalg.svd`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.svd.html) computes this decomposition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, S, VT = np.linalg.svd(Xmf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nicely enough, the returned $S$ is not a full matrix, but contains just the diagonal entries of $\\Sigma$ (the singular values). Instead of $V$, the function returns $V^T$ directly, such that `VT[i]` ($i$-th row of $V^T$) directly gives the $i$-th basis vector $v_i$ (the $i$-th column of $V$).\n",
    "\n",
    "Even nicer, `numpy.linalg.svd` returns the singular values in descending order; hence the principal component corresponds to the singular value $\\sigma_0$ (in `S[0]`), and $V$ is ordered accordingly, such that `VT[0]` is the first principal component.\n",
    "\n",
    "We can visually inspect the first few principal components by just visualizing them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Task 1:** Visualize the first 32 principal components similar to the first 24 faces using `plot_face`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4, 8, figsize=(16, 8))\n",
    "\n",
    "for ax, pc in zip(axes.flatten(), VT[:32]):\n",
    "    plot_face(ax, pc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is apparent, the main variance (corresponding to higher singular values) captures large-scale features, while with increasing $k$, column $v_k$ captures increasingly smaller details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Task 2**: Verify that $V$ is indeed an orthogonal matrix.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.allclose(np.dot(VT, np.transpose(VT)), np.identity(VT.shape[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partial reconstruction of a face using only the $K$ first basis vectors can be achieved as follows: let $f = X_j$ be one of the faces, then we want to write the reconstruction $f_K$ as the linear combination\n",
    "\n",
    "$$f_K\\ =\\ \\bar{X} + \\sum_{i=0}^{K-1} c_i v_i$$\n",
    "\n",
    "where the component $c_i$ of $f$ in direction $v_i$ is just given by the projection of $f-\\bar{X}$ onto $v_i$, $c_i = \\langle f-\\bar{X}, v_i \\rangle$. Turning to matrix notation, this simplifies to\n",
    "\n",
    "$$f_K\\ =\\ \\bar{X}\\ +\\ \\sum_{i=0}^{K-1} v_i c_i \\ =\\ \\bar{X}\\ +\\ \\sum_{i=0}^{K-1} v_i \\langle v_i, f-\\bar{X} \\rangle\\ = \\ \\bar{X}\\ +\\ \\sum_{i=0}^{K-1} v_i v_i^T (f-\\bar{X}),$$\n",
    "\n",
    "or, even simpler\n",
    "\n",
    "$$f_K\\ =\\ \\bar{X}\\ +\\ (v_0,\\dots,v_{K-1})^{\\phantom{T}} (v_0,\\dots,v_{K-1})^T (f-\\bar{X}),$$\n",
    "\n",
    "where $(v_0,\\dots,v_{K-1})$ is the (sub-)matrix consisting of the first $K$ columns of $V$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Task 3**: Implement the function `reconstruct` that computes the reconstruction $f_K$ given $f$ (`face`), `VT`, `Xmean`, and $K$.\n",
    "\n",
    "Note: below the function, there is code to do two things:\n",
    "\n",
    "1. Verify that the reconstruction works correctly by computing the full reconstruction $f_{1071}$ using all basis vectors, which shold be identical to the original image.\n",
    "\n",
    "2. Visualize the original face, as well as the reconstructed face using $K = 100$.\n",
    "\n",
    "If your code is correct, both of these should do sensible things.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct(face, VT, Xmean, K):\n",
    "    \"\"\"reconstruct face using K columns of VT and Xmean\"\"\"\n",
    "    \n",
    "    # Reconstructing the face using the weights and principal components\n",
    "    reconstructed_ = Xmean + np.dot(np.dot(face - Xmean, VT[:K].T), VT[:K])\n",
    "    return reconstructed_\n",
    "    \n",
    "    return np.zeros(face.shape)    \n",
    " \n",
    "\n",
    "# choose one face as a test image\n",
    "f = X[666,:]\n",
    "\n",
    "# verify that full reconstruction gives identical image\n",
    "if not np.allclose( reconstruct(f, VT, Xmean, 1071), f ):\n",
    "    print(\"full reconstruction DOES NOT match\")\n",
    "\n",
    "# visualize original and reconstruction\n",
    "fig, (ax0, ax1) = plt.subplots(1, 2, figsize=(6,3))\n",
    "\n",
    "plot_face(ax0, f)\n",
    "ax0.set_title(\"original\");\n",
    "\n",
    "plot_face(ax1, reconstruct(f, VT, Xmean, 100))\n",
    "ax1.set_title(\"reconstructed (100 comp.)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the original data is 4096-dimensional, with only $K=100$ principal components, we can achieve a passable reconstruction.\n",
    "\n",
    "Instead of guessing the right $K$ to achieve a passable reconstruction, we can proceed as follows. From the course, we know that the singular values $\\sigma_i$ are the square roots of the eigenvalues $\\lambda_i$ of the data's covariance matrix $(X-\\bar{X})^T (X-\\bar{X})$ in descending order. Furthermore, the fraction \n",
    "\n",
    "$$ \\frac{\\lambda_i}{\\lambda_1 + \\lambda_2 + \\dots + \\lambda_n} $$\n",
    "\n",
    "accounts for the overall variance of the data. We can determine the smallest $K$ such that enough variance is accounted for:\n",
    "\n",
    "$$\n",
    "\\min_{K}\n",
    "\\frac{\\lambda_1 + \\dots + \\lambda_K}{\\lambda_1 + \\dots + \\lambda_n}\\ =\\ \n",
    "\\min_{K}\n",
    "\\frac{\\sigma_1^2+\\dots+\\sigma_K^2}{\\sigma^2_1+\\dots+\\sigma^2_n} \n",
    "\\ >\\ C\n",
    "$$\n",
    "\n",
    "for some choice of $C$ (e.g. 95%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Task 4:** \n",
    "1. Write a function `var_K` to compute the accounted-for variance as a function of $K$, and visualize the result below in `ax0`.\n",
    "2. Write a function `min_K` to compute the minimum $K$ to account for 95% variance, and visualize the corresponding reconstruction as above.\n",
    "\n",
    "For both subtasks, the function [`numpy.cumsum`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.cumsum.html) will come in handy. Also, remember that `S` is the vector of singular values in descending order. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_K(S, K):\n",
    "    \"\"\"\n",
    "       return the fraction of variance accounted for by the first\n",
    "       K principal components, given the singular values S\n",
    "    \"\"\"\n",
    "    n = S.size\n",
    "    var = np.sum([S[i]**2 for i in range(K)])\n",
    "    totSum = np.cumsum(S**2)[n-1]\n",
    "    \n",
    "    return var / totSum\n",
    "    \n",
    "def min_K(S, C):\n",
    "    \"\"\"\n",
    "       return minimal K such that the fraction of variance accounted \n",
    "       for by the first K principal components exceeds C\n",
    "    \"\"\"\n",
    "    n = S.size\n",
    "    for i in range(n):\n",
    "        if (np.cumsum(S**2)[i] / np.cumsum(S**2)[n-1]) > C:\n",
    "            return i\n",
    "            \n",
    "    return n-1\n",
    "    \n",
    "\n",
    "Kmin = min_K(S, 0.95)\n",
    "\n",
    "# visualize and plot results\n",
    "fig, (ax0, ax1, ax2) = plt.subplots(1, 3, figsize=(9,3))\n",
    "\n",
    "K = range(len(S))\n",
    "V = [var_K(S, k) for k in K]\n",
    "ax0.plot( K, V )\n",
    "ax0.set_title(\"accounted variance fraction\")\n",
    "ax0.grid(True)\n",
    "\n",
    "plot_face(ax1, f)\n",
    "ax1.set_title(\"original\");\n",
    "\n",
    "plot_face(ax2, reconstruct(f, VT, Xmean, Kmin))\n",
    "ax2.set_title(\"reconstructed (K=%d)\" % Kmin);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "   \n",
    "   ### Randomized Singular Value Decomposition\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accurate and efficient computation of SVD of large data matrices might be computationally challenging. To overcome this issue, many algorithms have been developed by applying randomized linear algebra methods. One of the most significant of them is the randomized SVD algorithm, which is efficient for decomposing any large matrix with a relatively low rank. In the following, we will first briefly explain the method. The task is then to try it for an image compression problem. We start by loading a test image:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "    \n",
    "**Note:** Please download the file `test.jpg` from the OLAT materials folder, and put it in the same directory as this notebook.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = imread('test.jpg')\n",
    "plt.figure(figsize = (10,10))\n",
    "plt.imshow(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, we turn this image into a grayscale image by using the function `rgb2gray` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb2gray(rgb):\n",
    "\n",
    "    r, g, b = rgb[:,:,0], rgb[:,:,1], rgb[:,:,2]\n",
    "    gray = 0.2989 * r + 0.5870 * g + 0.1140 * b\n",
    "\n",
    "    return gray\n",
    "X = rgb2gray(A) # Convert RGB -> grayscale\n",
    "print('The image has:',X.shape[0],'x',X.shape[1],'pixels')\n",
    "plt.figure(figsize = (10,10))\n",
    "plt.imshow(X,cmap='gray', vmin=0, vmax=255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now apply the randomized SVD algorithm to the grayscale-image $m \\times n$ matrix `X`. The algorithm consists of the following steps:\n",
    "- Construct a $n \\times r$ random matrix $P$ to sample the column space of $X$. $r$ is the so-called target rank, which is supposed to be smaller than the rank of $X$.\n",
    "- Compute a new $m \\times r$ matrix $Y=X \\;P$.\n",
    "- Apply QR decomposition to the matrix $Y$ such that $Y = Q \\;R$.\n",
    "- With the low rank basis $Q$, we can now project $X$ into a smaller space $\\tilde X = Q^\\top X$.\n",
    "- Now we can apply SVD to $\\tilde X$, which is much smaller than $X$: $\\tilde X = \\tilde U \\tilde \\Sigma \\tilde V^\\top$.\n",
    "- Modify the left singular vectors $U = Q \\tilde U$. The approximate SVD is then given by $U, \\tilde \\Sigma,  \\tilde V^\\top$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Task 5**: Complete the function `randomizedSVD` below. The function inputs are the data matrix $X$ and the target rank $r$. The function should return the outputs $U$, $S$ and $V^\\top$, which approximate the standard SVD up to rank $r$. Use `np.random.randn` to generate the random matrix based on the standard normal distribution. You may use `np.linalg.qr` for the QR decomposition.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomized SVD function\n",
    "def randomizedSVD(X,r):\n",
    "    \n",
    "    # TO DO\n",
    "    return (np.linalg.svd(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Task 6**: Compare the run-time differences between the standard SVD and the randomized SVD (for $r= 200$) on the data matrix $X$ of the image using Jupyter's `%%timeit`. You may use the `np.linalg.svd` for the standard SVD.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO DO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Task 7**: Plot the compressed images by using the SVD-based image compression. One picture is compressed using the standard SVD and the other picture is compressed using the randomized SVD. Take the first $200$ singular values for the image reconstruction.  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
