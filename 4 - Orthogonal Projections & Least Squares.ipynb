{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The focus of this part (#4) are projections and least square regression.\n",
    "Let's start with initialization as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                # basic arrays, vectors, matrices\n",
    "import scipy as sp                # matrix linear algebra \n",
    "\n",
    "import matplotlib                 # plotting\n",
    "import matplotlib.pyplot as plt   # plotting\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"<style>.output_png { display: table-cell; text-align: center; vertical-align: middle; }</style>\"\"\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Projections in 3D\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture, we discussed how to project vectors onto subspaces. In this exercise we try the idea by projecting a 3D parametric curve on a plane. In general, parameteric curves are given as \n",
    "\n",
    "\\begin{eqnarray}\n",
    "x_1 & = & x_1(t) \\\\\n",
    "x_2 & = & x_2(t) \\\\\n",
    "x_3 & = & x_3(t)\n",
    "\\end{eqnarray}\n",
    "\n",
    "As an example, the equation of a helix on the cylinder $x_2^2 + x_3^2 = 1$ is given by\n",
    "\\begin{eqnarray}\n",
    "x_1 & = & t \\\\\n",
    "x_2 & = & \\cos(2t) \\\\\n",
    "x_3 & = & \\sin(2t)\n",
    "\\end{eqnarray}\n",
    "\n",
    "Below, we define such a helix and plot it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define the coordinates of a helix \n",
    "N = 1000\n",
    "t = np.linspace(0, 12, N)\n",
    "x1 = t\n",
    "x2 = np.cos(2*t)\n",
    "x3 = np.sin(2*t)\n",
    "plt.figure(figsize=(10,8))\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.scatter(x1, x2, x3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using parameterized curves, one can generate more fancy shapes: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 2456\n",
    "t = np.linspace(0, 12, N)\n",
    "x1 = 3/2*t + abs(t-1) - abs(t-3) + abs(t-4) + abs(t-7) - 2*abs(t-8) + 1.5* abs(t-11) \\\n",
    "   + 0.5 * (abs(t-4)/(t-4)  - 3*abs(t-6)/(t-6) + abs(t-7)/(t-7)  - 2*abs(t-8)/(t-8) ) \\\n",
    "   + 0.5 * (3*abs(t-9)/(t-9) +  3*abs(t-10)/(t-10) - 3*abs(t-11)/(t-11)   )   - 37/2\n",
    "\n",
    "\n",
    "x2 = 2*t - 3*abs(t-1) + 2*abs(t-2) - 3*abs(t-3) + 4*abs(t-4) - 4*abs(t-5) + 2*abs(t-6) - 2*abs(t-8)+ 2*abs(t-11) \\\n",
    "   + abs(t-6)/(t-6)+ abs(t-7)/(t-7) + 2*abs(t-9)/(t-9) + 2*abs(t-10)/(t-10) +abs(t-11)/(t-11)-1\n",
    "\n",
    "x3 = N*[1.0]\n",
    "plt.figure(figsize=(10,8))\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.scatter(x1, x2, x3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, the coordinate of a point on a plane in 3D coordinate system is given by\n",
    "\\begin{equation}\n",
    "\\mathbf{x} = (x_1, x_2, x_3)^\\top = \\mathbf{r_0} + \\alpha \\mathbf{v_1} + \\beta \\mathbf{v_2},\n",
    "\\end{equation}\n",
    "where $\\mathbf{r_0},\\mathbf{v_1},\\mathbf{v_2}$ are vectors and $\\alpha, \\beta$ are real numbers. Note that the vectors $\\mathbf{v_1},\\mathbf{v_2}$ span a subspace (a plane), which goes through the origin. The vector $\\mathbf{r_0}$ shifts that plane in the coordinate sytem.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Task**: Complete the function `project_points_on_a_surface` below to implement the projection on a plane, given the coordinate matrix `C`, which holds the three coordinates of each point in its three rows. Note that each point is a column of this matrix. The function should have the following properties:\n",
    "- The function should check whether v1 and v2 are linearly independent. If this is not the case it should terminate with error.\n",
    "- Within the function body, a projection matrix P should be calculated (using the input arguments of the function). This matrix is then used to perform the projection.\n",
    "- The function should return the coordinates of the projected points as a matrix. The input paramaters of the function are the coordinate matrix of input points, the vectors r0,v1 and v2 that are used to parameterize the projection plane. \n",
    "- Note: You may use standard functions like numpy.linalg.inv\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "# we fill the matrix C with coordinate arrays \n",
    "C = np.array([x1,x2,x3])\n",
    "\n",
    "def project_points_on_a_surface(C, r0, v1, v2):\n",
    "    \"\"\"project the curve onto the surface parameterized by v1 and v2\"\"\"    \n",
    "\n",
    "    #check if v1 and v2 are linearly independent\n",
    "    if ( v1[0]/v2[0] == v1[1]/v2[1] == v1[2]/v2[2] ):\n",
    "        raise Exception(\"vectors not linearly independent\")\n",
    "    \n",
    "    #Calculate projection matrix P using v1 and v2\n",
    "    A = np.array([[v1[0],v2[0]],[v1[1],v2[1]],[v1[2],v2[2]]])\n",
    "    P = np.matmul(A, np.matmul((np.linalg.inv(np.matmul(np.transpose(A),A))), np.transpose(A)))\n",
    "    C = np.matmul(P,C+r0.reshape(3,1))     #add support vector as offset\n",
    "#     C += r0.reshape(3,1)                   #add offset again\n",
    "    \n",
    "    return C\n",
    "\n",
    "\n",
    "r0 = np.array([0,0,1])\n",
    "v1 = np.array([1,2,3])\n",
    "v2 = np.array([1,1,1])\n",
    "\n",
    "\n",
    "Cprojected = project_points_on_a_surface(C,r0,v1,v2)\n",
    "\n",
    "# now we plot the projected coordinates along with the original coordinates \n",
    "plt.figure(figsize=(10,8))\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.scatter(C[0,:], C[1,:], C[2,:])\n",
    "ax.scatter(Cprojected[0,:], Cprojected[1,:], Cprojected[2,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "### Least Squares Regression \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed in the course, the least squares method can be used to solve overdetermined systems (these have more equations than unknowns). The method has usage in many applications, especially in statistics and machine learning. In this exercise, we try it in the context of machine learning (ML).\n",
    "\n",
    "In this task, the goal is to develop a predictive ML model based on the so-called Boston Housing data. The dataset can be found in the UCI Machine Learning Repository. The data was collected in 1970s and each entry (each row in the data) represents information about different features of homes from various suburbs located in Boston. Let's load the data and take a first look.\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "    \n",
    "**Note:** Please download the file `Boston_Housing_Data.csv` from OLAT, and put it in the same directory as this notebook.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import genfromtxt\n",
    "housingData = genfromtxt('Boston_Housing_Data.csv', delimiter=',')\n",
    "NumberOfSamples = housingData.shape[0]\n",
    "NumberOfFeatures = housingData.shape[1]\n",
    "print('Boston Housing Data:',housingData)\n",
    "print('The data has', NumberOfSamples, 'samples.')\n",
    "print('Each sample has', NumberOfFeatures, 'features')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading, we can observe that the `housingData` array contains 506 samples (rows). Each sample represents a house and has 14 features (columns in the data). The features are like:\n",
    "- capita crime rate \n",
    "- index of accessibility to radial highways\n",
    "- pupil-teacher ratio by town\n",
    "- ...\n",
    "\n",
    "and so on. \n",
    "\n",
    "What is most important to us is the last feature (last column in the data), namely the price of the house in thousand dollars (do not surprise much, these are 70s prices!). Our aim is to develop a model, which can predict the value of any house. For this exercise, we choose the linear model for the price denoted by $p$:\n",
    "\n",
    "\\begin{equation}\n",
    "p(\\mathbf{x}) = w_0 + w_1 * x_1 + w_2 * x_2 + \\ldots + w_{13}* x_{13}, \n",
    "\\end{equation}\n",
    "\n",
    "where the vector $\\mathbf{w} = (w_0,w_1,\\ldots, w_{13})^\\top$ are the weights of the model and the vector $\\mathbf{x} = (x_1,x_2, \\ldots, x_{13})^\\top$ is the features vector. To account for the bias term $w_0$, we can add $x_0=1$ to the features vector such that the price function is simply a scalar product:\n",
    "\n",
    "\\begin{equation}\n",
    "p(\\mathbf{x}) = w_0 * x_0 + w_1 * x_1 + w_2 * x_2 + \\ldots + w_{13}* x_{13} = \\mathbf{w}^\\top \\mathbf{x}, \n",
    "\\end{equation}\n",
    "\n",
    "Our price model can predict the value of any house if its other $13$ features are known. The crucial issue is now to adjust the model weights in such way that our model performs well for the new samples, which are not in the data. To find the best values for the weights, we take $N$ sample points from the data (these we call training samples in the following) and try to fit the linear price function such that it provides exact interpolations at those points. Therefore, for each sample we can write a linear equation:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "w_0 * x_0^{(1)} + w_1 * x_1^{(1)} + w_2 * x_2^{(1)} + \\ldots + w_{13}* x_{13}^{(1)} & = & p(\\mathbf{x}^{(1)}) \\\\\n",
    "w_0 * x_0^{(2)} + w_1 * x_1^{(2)} + w_2 * x_2^{(2)} + \\ldots + w_{13}* x_{13}^{(2)} & = & p(\\mathbf{x}^{(2)}) \\\\\n",
    "\\vdots \\quad \\quad \\quad \\quad \\vdots & & \\vdots \\\\\n",
    "w_0 * x_0^{(N)} + w_1 * x_1^{(N)} + w_2 * x_2^{(N)} + \\ldots + w_{13}* x_{13}^{(N)} & = & p(\\mathbf{x}^{(N)})\n",
    "\\end{eqnarray}\n",
    "\n",
    "The superscript $(i)$ denotes the index of the particular sample, e.g., $x_3^{(11)}$ means the $x_3$ from the $11$th sample. In matrix form we can write the above linear system as\n",
    "\n",
    "\\begin{equation}\n",
    "X \\mathbf{w} = \\mathbf{p},\n",
    "\\end{equation}\n",
    "\n",
    "with the coefficient matrix\n",
    "\n",
    "\\begin{equation}\n",
    "X=\n",
    "\\begin{bmatrix}\n",
    "1 & x_1^{(1)} & x_2^{(1)} & \\ldots & x_{13}^{(1)} \\\\\n",
    "1 & x_1^{(2)} & x_2^{(2)} & \\ldots & x_{13}^{(2)} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "1 & x_1^{(N)} & x_2^{(N)} & \\ldots & x_{13}^{(N)}\n",
    "\\end{bmatrix}.\n",
    "\\end{equation}\n",
    "\n",
    "Note that for $N>14$ this linear system is overdetermined and it is very unlikely that we can find a solution in the classical sense. However, as we have learnt in the lecture, overdetermined systems can be solved using the least squares method. Finding this least square solution is precisely the task in this exercise.\n",
    "\n",
    "As you might have noted, we have in total $506$ samples in the data. We use $N=400$ samples for the linear system. The rest $106$ samples will be used to evaluate the performance our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 400\n",
    "trainingData = housingData[0:N,:]\n",
    "testData = housingData[N:NumberOfSamples,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# J = np.array([[0.00632,18,2.31,0.0,0.538,6.575,65.2],[4.09,1,296,15.3,396.9,4.98,24],\n",
    "# [0.02731,0,7.07,0.0,0.469,6.421,78.9],[4.9671,2,242,17.8,396.9,9.14,21.6]])\n",
    "# x = J[:,-1:]\n",
    "# y = J[2:, :4]\n",
    "\n",
    "# print(x.shape)\n",
    "# print(y.shape)\n",
    "# print(y@x)\n",
    "\n",
    "# print(J.shape)\n",
    "# print(J[:, -1:])\n",
    "\n",
    "# TD = np.zeros(trainingData.shape)\n",
    "# TD[:, 0] = 1 # to account for the bias term, we add ones at the first column in each row\n",
    "# TD[:, 1:] = trainingData[:, :-1] # here, -1, to get all the columns,  excluding the last one since it contains the prices. \n",
    "# print((np.linalg.inv(TD.T@TD)@TD.T).shape)\n",
    "\n",
    "# P = trainingData[:,-1:] # we grab the last column for each rows in the trainig Data\n",
    "# print(P.shape)\n",
    "# weights = np.linalg.inv(TD.T @ TD) @ TD.T @ P # the formula from the slides\n",
    "# print (weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Task**: Implement the functions `find_weights` and `estimate_price` below\n",
    "- The function `find_weights` should find the least square solution for the problem $X\\mathbf{w} = \\mathbf{p}$\n",
    "- The function `estimate_price` should simply return a price value for given $\\mathbf{w}$ and $\\mathbf{x}$.\n",
    "- Note: You may use standard functions like the numpy.linalg.inv but do not use numpy.linalg.lstsq\n",
    "- Note: Do not forget the column of ones in the $X$ matrix (for the bias term)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_weights(trainingData):\n",
    "    TD = np.zeros(trainingData.shape)\n",
    "    TD[:, 0] = 1 # to account for the bias term, we add ones at the first column in each row\n",
    "    TD[:, 1:] = trainingData[:, :-1] # here, -1, to get all the columns,  excluding the last one since it contains the prices. \n",
    "   \n",
    "    P = trainingData[:,-1:] # we grab the last column for each rows in the trainig Data\n",
    "    weights = np.linalg.inv(TD.T @ TD) @ TD.T @ P # the formula from the lecture slides\n",
    "    \n",
    "    \n",
    "    return weights\n",
    "\n",
    "def estimate_price(weights,x):\n",
    "    \n",
    "    Eprice = x@weights\n",
    "    \n",
    "    return Eprice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to try our model with the test data. First we calculate the weight vector by calling the function \n",
    "`find_weights` with the `trainingData`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = find_weights(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, we try our model on the `testData` and calculate the mean squared error (MSE) of our estimations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE = 0.0\n",
    "for sample in testData:\n",
    "  x = sample[0:NumberOfFeatures-1]  \n",
    "  x = np.append([1.0],x)                \n",
    "                               \n",
    "  estimatedPrice = estimate_price(weights,x)\n",
    "  truePrice = sample[NumberOfFeatures-1]  \n",
    "  print('estimated price = ',estimatedPrice, 'true price = ',truePrice)\n",
    "  MSE = MSE + (estimatedPrice-truePrice)**2.0  \n",
    "\n",
    "MSE = MSE/(NumberOfSamples-N)\n",
    "print('Mean Squared Error for the test data = ',MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
