{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part focuses on optimization of multivariate functions. Lets start with the initializations as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (5.0, 5.0)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "## Derivatives of Multivariate Functions\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture, we have discussed gradient and Hessian of multivariate functions. Let's consider the following function $f:\\mathbb{R}^2 \\rightarrow \\mathbb{R}$ defined by\n",
    "\n",
    "$$\n",
    "f(x,y) = \\sin(x^2) - \\cos(y)^4 + \\frac{1}{2}xy\n",
    "$$\n",
    "\n",
    "We can plot the graph of this function, i.e. the surface over the $(x,y)$-plane whose height is given by $f(x,y)$, over the rectangle $[-1,1]^2$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x, y):\n",
    "    \"\"\"evaluate f at (x,y)\"\"\"\n",
    "    return np.sin(x**2) - np.cos(y)**4 + 0.5*x*y\n",
    "\n",
    "# define a grid \n",
    "X, Y = np.mgrid[-1:1:50j,-1:1:50j]\n",
    "\n",
    "# make a surface plot of the graph of f\n",
    "fig = plt.figure(figsize=(7,7))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.plot_surface(X, Y, func(X, Y), cmap=\"viridis\"); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking closely, it appears that $f$ has a minimum at $(0,0)$. Let's verify that this is really the case. To do so, we need two ingredients:\n",
    "\n",
    "1. The gradient $\\nabla f(x,y) = (\\frac{\\partial f}{\\partial x}(x,y), \\frac{\\partial f}{\\partial y}(x,y))$, which gives the local rates of change in the coordinate direction, and can also be interpreted as the direction of steepest ascent. A necessary condition for a minimum (or any other extremal point) at $(x,y)$ is that\n",
    "\n",
    "$$\n",
    "\\nabla f(x,y) = 0.\n",
    "$$\n",
    "\n",
    "2. If the gradient is zero, a sufficient condition for a minimum in $(x,y)$ is that the Hessian matrix of $f$,\n",
    "$$\n",
    "H_f(x,y) = \\begin{pmatrix} \n",
    "\\frac{\\partial^2 f}{\\partial x\\partial x}(x,y) &\n",
    "\\frac{\\partial^2 f}{\\partial x\\partial y}(x,y) \\\\\n",
    "\\frac{\\partial^2 f}{\\partial y\\partial x}(x,y) &\n",
    "\\frac{\\partial^2 f}{\\partial y\\partial y}(x,y)\n",
    "\\end{pmatrix},\n",
    "$$\n",
    "is positive definite. Since it is symmetric ($\\frac{\\partial^2 f}{\\partial x\\partial y} = \\frac{\\partial^2 f}{\\partial x\\partial y}$), this is equivalent to all of its eigenvalues being positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Task 1:** Verify, **analytically**, that $(0,0)$ is indeed a minimum of $f$.\n",
    "\n",
    "Steps:\n",
    "- Compute gradient and Hessian matrix of $f$ analytically.\n",
    "- Validate that $\\nabla f(0,0) = 0$.\n",
    "- Evaluate $H_f(0,0)$ and compute its eigenvalues, by finding the roots of its characteristic polynomial.\n",
    "- Check that all eigenvalues are positive.\n",
    "\n",
    "Note: there is no implementation in this task -- use pen and paper, or a symbolic algebra tool of your choice.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "\n",
    "Gradient:\n",
    "$$\n",
    "\\nabla f(x,y) = \\begin{pmatrix}\n",
    "2x\\cdot cos(x^2) + \\frac{1}{2}y \\\\\n",
    "4 cos(y)^3sin(y) + \\frac{1}{2}x)\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Gradient at $(0,0)$:\n",
    "$$\n",
    "\\nabla f(0,0) = (0,0)\n",
    "$$\n",
    "\n",
    "Hessian:\n",
    "$$\n",
    "H_f(x,y) = \\begin{pmatrix}\n",
    "2cos(x^2)-4x^2sin(x^2) & \\frac{1}{2} \\\\\n",
    "\\frac{1}{2} & 4cos^4(y) - 12cos^2(y)sin^2(y)\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Hessian at $(0,0)$:\n",
    "$$\n",
    "H_f(0,0) = \\begin{pmatrix}\n",
    "2 & \\frac{1}{2} \\\\\n",
    "\\frac{1}{2} & 4\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Characteristic polynomial of $H_f(0,0)$:\n",
    "$$\n",
    "\\chi(H_f(0,0))\\ =\\ \\lambda^2 - 2\\lambda - \\frac{31}{4}\n",
    "$$\n",
    "\n",
    "Roots of $\\chi(H_f(0,0))$:\n",
    "$$\n",
    "\\lambda_{1} = 1.881966\n",
    "$$\n",
    "$$\n",
    "\\lambda_{2} = 4.118034\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**Task 2:** Verify, **numerically**, that $(0,0)$ is indeed a minimum of $f$.\n",
    "\n",
    "Steps:\n",
    "- Implement functions for gradient (`gradient`) and Hessian (`Hessian`) of $f$ using the formulas from Task 1.\n",
    "- Pass the assertions to validate that the gradient vanishes and the Hessian is positive definite.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(x, y):\n",
    "    \"\"\"evaluate the gradient of f at (x,y)\"\"\"\n",
    "    dx = 2*x*np.cos(x**2) + y/2.0\n",
    "    dy = 4*np.cos(y)**3*np.sin(y) + x/2.0\n",
    "    return np.array([dx, dy])\n",
    "\n",
    "def hessian(x, y):\n",
    "    \"\"\"evaluate the Hessian matrix of f at (x,y)\"\"\"\n",
    "    dx = 2*np.cos(x**2) - 4*x**2*np.sin(x**2)\n",
    "    dy = 4*np.cos(y)**4 - 12*np.cos(y)**2*np.sin(y)**2\n",
    "    xdy = 1/2.0\n",
    "    return np.array([[dx, xdy], [xdy, dy]])\n",
    "\n",
    "# TODO: verify (numerically) that $(0,0)$ is a minimum\n",
    "assert np.allclose(gradient(0, 0), 0.0)\n",
    "\n",
    "eig, _ = np.linalg.eig(hessian(0, 0))\n",
    "assert np.all( eig > 0 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at two ways to visualize $f$ and its derivative.\n",
    "    \n",
    "One possibility is to use the contour plot. A *contour* or *level set* is the pre-image (German: *Urbild*) of some $v\\in\\mathbb R$ under $f$. In other words, given a value $v$, we can identify the set of all points $(x,y)$ such that $f(x,y) = v$. More formally, the level set is \n",
    "\n",
    "$$\n",
    "I_f(v) := \\big\\{\\ (x,y) \\in \\mathbb{R}^2\\ |\\ f(x,y) = v\\ \\big\\}.\n",
    "$$\n",
    "\n",
    "Matplotlib's `contour` function takes a grid of sample points for $x$ and $y$, as well as the function value $f(x,y)$ at these points, and computes $I_f(v)$ for a set of suitably chosen values of $v$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.contour(X, Y, func(X, Y));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contours are quite analogous to height lines on a geographical map.\n",
    "\n",
    "To visualize the gradients, we can use the `pyplot.quiver` function, which draws arrows at a selected set of points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Task 3:** Use the `pyplot.quiver` function to visualize $\\nabla f$ on a $25\\times 25$-grid over $[-1,1]^2$, on top of the contours.\n",
    "\n",
    "Hint: Use `numpy.mgrid` as above to obtain the grid.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the grid\n",
    "x_pts, y_pts = np.mgrid[-1:1:25j, -1:1:25j]\n",
    "# get the values from calculating the gradient\n",
    "x, y = gradient(x_pts, y_pts)\n",
    "# plot function ontop of the contours\n",
    "plt.contour(X, Y, func(X, Y))\n",
    "# visualize f\n",
    "plt.quiver(x_pts, y_pts, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "## Numerical Optimization - Steepest Descent Method\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture, we have seen that the steepest descent method can be used to find the minimum of a function.\n",
    "The task in this exercise is to analyze the steepest descent method to $f$: starting from some initial position $x_0,y_0$, we choose a step size $\\alpha$ and perform the iteration\n",
    "\n",
    "$$\n",
    "(x_{i+1}, y_{i+1}) = (x_i,y_i) - \\alpha \\nabla f(x_i, y_i),\n",
    "$$\n",
    "\n",
    "i.e. we move along the reverse gradient to decrease the function value. The iteration is terminated once the change between successive iterates becomes small, i.e. when $\\nabla f(x_i,y_i) < \\varepsilon$ for some small $\\varepsilon > 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Task 4:** Write a function `gradient_descent` below that performs the gradient descent procedure from the initial position $x_0,y_0$ and returns the sequence of the $(x_i,y_i)$. Use the provided visualization to validate that the iteration reaches $f$'s minimum at $(0,0)$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x0, y0, alpha = 0.1, eps=1e-6):\n",
    "    \"\"\"perform gradient descent starting at x0,y0, with step size alpha and error tolerance eps.\"\"\"\n",
    "    path = [(x0, y0)]\n",
    "    \n",
    "    grad = gradient(x0,y0)\n",
    "    \n",
    "    while np.any(abs(grad) > eps):\n",
    "        x0 = x0 - alpha * grad[0]\n",
    "        y0 = y0 - alpha * grad[1]\n",
    "        path.append((x0,y0))\n",
    "        grad = gradient(x0,y0)\n",
    "        \n",
    "    return np.array(path)\n",
    "\n",
    "\n",
    "plt.contour(X, Y, func(X, Y), colors='gray', linestyles='solid', linewidths=.5)\n",
    "\n",
    "path = gradient_descent(0.1, 0.9)\n",
    "plt.plot( path[:,0], path[:,1], 'r');\n",
    "\n",
    "print(\"Number of iterations:\", len(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do an experiment: we perform the gradient descent for $(x_0,y_0)$ on a uniformly spaced grid, and record the path for every choice of starting point. Then, we plot all of these paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X0, Y0 = np.mgrid[-1:1:25j,-1:1:25j]\n",
    "\n",
    "paths = []\n",
    "\n",
    "for x0, y0 in zip(X0.ravel(), Y0.ravel()):\n",
    "    paths.append( gradient_descent(x0, y0) )\n",
    "\n",
    "plt.contour(X, Y, func(X, Y), colors='gray', linestyles='solid', linewidths=.5)\n",
    "\n",
    "for path in paths:\n",
    "    plt.plot( path[:,0], path[:,1], 'r', lw=1, alpha=0.5 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going one step further, we can color-code the lengths of the paths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.contour(X, Y, func(X, Y), colors='gray', linestyles='solid', linewidths=.5)\n",
    "\n",
    "minlen = min([len(p) for p in paths])\n",
    "maxlen = max([len(p) for p in paths])\n",
    "print( \"minimal path length:\", minlen )\n",
    "print( \"maximal path length:\", maxlen )\n",
    "\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.cm as cmx\n",
    "\n",
    "cmap = plt.get_cmap('viridis')\n",
    "cnorm  = colors.Normalize(vmin=minlen, vmax=maxlen)\n",
    "smap = cmx.ScalarMappable(norm=cnorm, cmap=cmap)\n",
    "\n",
    "for path in paths:\n",
    "    plt.plot( path[:,0], path[:,1], 'r', lw=1, alpha=0.5, color=smap.to_rgba(len(path)))\n",
    "    \n",
    "plt.colorbar(smap);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, some of the paths converge fairly quickly, other take much longer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "## Numerical Optimization - Newton's Method\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture, we have seen that the Newton's method converges faster than the steepest descent method. The task in this exercise is to try it for finding the minimum of $f(x,y)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**Task 5:** Write a function `Newton_method` that computes the minimum of $f$ at $(x,y)$ using the damped Newton method. Try different values of $\\alpha$ to find the setting which gives a good convergence. Compare your results with those that are obtained by the steepest-descent method.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wiki https://en.wikipedia.org/wiki/Gauss%E2%80%93Newton_algorithm\n",
    "def Newton_method(x0, y0, alpha = 0.1, eps=1e-6): # alpha Schrittweite, eps Fehlergrenze\n",
    "    \"\"\"\n",
    "    Führt die gedämpfte Newton-Methode aus.\n",
    "    \n",
    "    Parameter:\n",
    "    x0, y0 (Float): Anfangsschätzung für x und y.\n",
    "    alpha (Float): Dämpfungsfaktor.\n",
    "    eps (Float): Fehlertoleranz für die Anhaltebedingung.\n",
    "    \n",
    "    Rückgabe:\n",
    "    numpy.ndarray: Array von Iteraten (x, y).\n",
    "    \"\"\"\n",
    "    # Definiere die Funktion f und die Jacobian J\n",
    "    def f(x):\n",
    "        return np.array([x[0]**2 - x[1], x[1]**2 - x[0]])\n",
    "    \n",
    "    def J(x):\n",
    "        return np.array([[2*x[0], -1], [-1, 2*x[1]]])\n",
    "    \n",
    "    # Initialisiere den Startpunkt und path\n",
    "    x = np.array([x0, y0])\n",
    "    path = [x.copy()]\n",
    "    \n",
    "    while True:\n",
    "        # Berechne den nächsten Schritt\n",
    "        s = np.linalg.inv(J(x)).dot(f(x))\n",
    "        x_next = x - alpha * s\n",
    "        path.append(x_next.copy())\n",
    "        \n",
    "        # Prüfe die Endkondition\n",
    "        if np.linalg.norm(f(x_next)) < eps:\n",
    "            break\n",
    "        \n",
    "        x = x_next\n",
    "    \n",
    "    return np.array(path)\n",
    "\n",
    "plt.contour(X, Y, func(X, Y), colors='gray', linestyles='solid', linewidths=.5)\n",
    "\n",
    "path = Newton_method(0.1, 0.4)\n",
    "plt.plot( path[:,0], path[:,1], 'r');\n",
    "\n",
    "print(\"Number of iterations:\", len(path))\n",
    "\"\"\" \n",
    "Die Abbildung zeigt die Konturlinien der Funktion f(x,y) in Grau \n",
    "und den Pfad der Iterationen des Newton-Verfahrens in Rot. \n",
    "Der Beginn ist bei dem Punkt (0.1, 0.4)(0.1,0.4). \n",
    "Der rote Pfad verfolgt die Iterationen, wie sie sich dem Minimum der Funktion annähern.\n",
    "\n",
    "Verglichen mit steepest-descent method:\n",
    "Bei der steepest-descent method werden 67 Iterationen benötigt, bei Newton's Method 124.\n",
    "\"\"\" \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
