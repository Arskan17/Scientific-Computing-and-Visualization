{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part (#5) is about $QR$ factorization via the Gram-Schmidt process and eigenvalues/eigenvectors. Let's start with initializations as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                # basic arrays, vectors, matrices\n",
    "import scipy as sp                # matrix linear algebra \n",
    "\n",
    "import matplotlib                 # plotting\n",
    "import matplotlib.pyplot as plt   # plotting\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"<style>.output_png { display: table-cell; text-align: center; vertical-align: middle; }</style>\"\"\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Gram-Schmidt Process\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the course, we discussed how the Gram-Schmidt process can be used for the QR decomposition of a matrix. In this exercise, we try this idea by applying it for the solution of a linear system. We can start by generating a random square matrix $A$: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5\n",
    "A = np.random.rand(N,N)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Task 1**: Complete the function `factorize_QR` below to implement the QR factorization of a square matrix by the Gram-Schmidt process. The function should have the following properties:\n",
    "\n",
    "- For the given input matrix $A$, it should return the matrices $Q$ and $R$.\n",
    "- The diagonal of $R$ should have positive entries.\n",
    "- It should terminate with error if the columns of $A$ are not linearly independent.\n",
    "- Note 1: You may use a standard implementation for the scalar product and norm of a vector.\n",
    "- Note 2: For an orthogonal matrix $Q$ it applies $Q^\\top Q = Q Q^\\top =I$. You may use it to check the correctness of your implementation.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wertvolle Seite https://zerobone.net/blog/cs/gram-schmidt-orthogonalization/\n",
    "def factorize_QR(A):\n",
    "    m = A.shape[0]\n",
    "    Q = np.copy(A)\n",
    "    R = np.zeros((m,m))\n",
    "    for i in range(m):\n",
    "# i-te Spalte von A. \n",
    "        R[i, i] = np.linalg.norm(Q[:, i]) \n",
    "# Test ob bei der Diagonalen eine 0 dabei ist.\n",
    "        if np.isclose(R[i, i], 0):\n",
    "            raise np.linalg.LinAlgError(\"Column of vectors != linearly independent\")\n",
    "# Normalisiere die Spalten i von Q        \n",
    "        Q[:, i] = Q[:, i] / R[i, i]  \n",
    "        \n",
    "        for j in range(i + 1, m):\n",
    "            R[i, j] = np.dot(Q[:, i], A[:, j])  # Compute the non-diagonal elements of R\n",
    "            Q[:, j] = Q[:, j] - R[i, j] * Q[:, i]  # Update column j of Q\n",
    "    \n",
    "    return Q, R\n",
    "\"\"\"\n",
    "Weiterer Versuch\n",
    "for j in range(i):\n",
    "        R = R - np.dot(Q[:, j], Q[:, i]) * Q[:, j]\n",
    "    \n",
    "    if np.array_equal(R, np.zeros(R.shape)):\n",
    "        raise np.linalg.LinAlgError(\"The column vectors are not linearly independent\")\n",
    "    \n",
    "    # Normalisiere q.\n",
    "    R = R / np.sqrt(np.dot(R, R))\n",
    "    \n",
    "    # Schreibe den normalisierten Vektor zurück in Q.\n",
    "    Q[:, i] = R # Make a copy of A to preserve its original values\n",
    "return Q, R \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "Q,R = factorize_QR(A)\n",
    "print(\"\\nMatrix A:\\n\", A)\n",
    "print(\"\\nMatrix Q:\\n\", Q)\n",
    "print(\"\\nMatrix R:\\n\", R)\n",
    "# Checking ob Q^T * Q = I. np.allclose: two arrays are element-wise equal within a tolerance.\n",
    "print(\"\\nCheck Q^T * Q == I:\\n\", np.allclose(np.dot(Q.T, Q), np.identity(N)))\n",
    "\n",
    "\"\"\" \n",
    "# Anderes Bsp.\n",
    "N = 7\n",
    "B = np.random.rand(N, N)\n",
    "print(\"Matrix B:\\n\", A)\n",
    "\n",
    "C, D = factorize_QR(B)\n",
    "print(\"\\nMatrix C:\\n\", C)\n",
    "print(\"\\nMatrix D:\\n\", D)\n",
    "\n",
    "# Checking if Q^T * Q = I (approximately)\n",
    "print(\"\\nC^T * C:\\n\", np.dot(C.T, C))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having implemented the QR factorization, we can use it to solve a linear system $A \\mathbf{x} = \\mathbf{b}$. For the test, we generate a random right hand side vector $\\mathbf{b}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.random.rand(N,1)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we substitute $A = QR$ into the linear system we get \n",
    "\\begin{equation}\n",
    "QR \\mathbf{x} = \\mathbf{b}.\n",
    "\\end{equation}\n",
    "If we multiply both sides with $Q^{-1}$ from the left, we get\n",
    "\\begin{equation}\n",
    "Q^{-1} QR \\mathbf{x} = Q^{-1} \\mathbf{b}.\n",
    "\\end{equation}\n",
    "Since $Q^{-1} Q = I$, we have\n",
    "\\begin{equation}\n",
    "R \\mathbf{x} = Q^{-1} \\mathbf{b},\n",
    "\\end{equation}\n",
    "which can be solved easily by the backward substitution method that you have already implemented in the previous exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Task 2**: Complete the function `solve_with_QR` below, which solves a linear equation system $A\\mathbf{x} = \\mathbf{b}$ using the QR factorization. \n",
    "\n",
    "- For the given QR factorization of $A$, it should return the solution $\\mathbf{x}$.\n",
    "- Note 1: You may use a standard implementation for the matrix-matrix product.\n",
    "- Note 2: It should be clear by now how to invert $Q$ easily.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_with_QR(Q,R,b):\n",
    "# Erstellung Nullvektor x mit gleicher Größe wie b.\n",
    "    x = np.zeros_like(b) \n",
    "# Berechnung Skalarprodukt (transponierten Matrix Q und b).\n",
    "    Qt_b = np.dot(Q.T, b)\n",
    "    n = R.shape[0]  \n",
    "# Iteriert von der vorletzten bis zur ersten Zeile von R in umgekehrter Reihenfolge.\n",
    "    for i in range(n - 1, -1, -1):  \n",
    "        x[i] = (Qt_b[i] - np.dot(R[i, i + 1:], x[i + 1:])) / R[i, i]\n",
    "# Berechnet die i-te Komponente des Lösungsvektors x basierend auf den vorher berechneten Komponenten.\n",
    "# Die QR-Faktorisierung für die Lösung des linearen Gleichungssystems.\n",
    "    return x\n",
    "\n",
    "b = np.random.rand(N, 1)\n",
    "# QR factorization\n",
    "Q, R = np.linalg.qr(A)\n",
    "\n",
    "# Löse die Gleichung Ax = b mit QR factorization.\n",
    "x_solution = solve_with_QR(Q, R, b)\n",
    "print(\"Lösung x:\\n\", x_solution)\n",
    "\n",
    "# Check die Lösung durchs Vergleichen von Ax und b.\n",
    "computed_b = np.dot(A, x_solution)\n",
    "print(\"\\nb (Ax):\\n\", computed_b)\n",
    "print(\"\\nOriginal b:\\n\", b)\n",
    "print(\"\\nCheck ob Ax gleich zu b ist:\\n\", np.allclose(computed_b, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we check the implementation by calculating the residual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = solve_with_QR(Q,R,b)\n",
    "r = b - np.matmul(A,x)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the Gram-Schmidt process is easy to understand and implement, it may suffer from numerical errors for larger matrices. Below we demonstrate such a case, for which the Gram-Schmidt process shows an unstable behaviour. We first generate a random square matrix $A$ with $n=50$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 50;\n",
    "A = np.random.rand(N,N)\n",
    "Q, R = np.linalg.qr(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After applying the QR factorization to $A$, we modify then the diagonal elements of $R$ such that they are decaying exponents of $2$: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we modify the main diagonal of R\n",
    "for i in range(N):\n",
    "    R[i,i] = 2**(-i)\n",
    "print(R.diagonal())    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then reconstruct a modified $A$ matrix from $Q$ and $R$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = Q@R\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Task 3**: Apply your QR factorization to the modified matrix $A$. Compare the exact values of the main diagonal $R$ with the values obtained from the Gram-Schmidt process in a plot. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.diag(R) aus Box 7\n",
    "R_diagonal = np.diag(R)\n",
    "\n",
    "# Anwenden unserer Funktion\n",
    "Q, R = factorize_QR(A)\n",
    "\n",
    "# mit dem R von dem modifizierten\n",
    "R_diagonal_with_QR = np.diagonal(R)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Plotting comparison\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(np.abs(R_diagonal), label='Genaue Werte von numpy QR')\n",
    "plt.plot(np.abs(R_diagonal_with_QR), 'ro', label='Werte von Gram-Schmidt')\n",
    "plt.title('Vergleich der diagonalen Werte von R')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Absoluter Wert')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "### Graph Laplacian and Spectral Clustering\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Via adjacency matrices, graphs have a deep connection to linear algebra. In this exercise, the goal is to use eigenvalues and eigenvectors of the so-called *Graph Laplacian* matrix to do some data analysis. Specifically, we will consider clustering, i.e. partitioning a data set into disjoint classes.\n",
    "\n",
    "Let's load a test data set consisting of two-dimensional points to illustrate the problem.\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "    \n",
    "**Note:** Please download the file `pointcloud.npz` from the OLAT materials folder, and put it in the same directory as this notebook.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load(\"pointcloud.npz\")[\"data\"]\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter( X[:,0], X[:,1], 6.0, color=\"crimson\" )\n",
    "plt.axis(\"equal\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The challenge in clustering is to separate meaningful classes among the data items; in this case, we would like to separate the points forming the \"S\" in the middle from the points forming the surrounding circle. We will use a technique called **spectral clustering** to achieve this.\n",
    "\n",
    "The first step is to create an undirected graph $G = (X,E)$ from the data points as follows. $X$ are the vertices of the graph, whose two coordinates are stored in the two columns of the matrix `X`. There is an edge connecting any two (different) points whose (Euclidean) distance is lower than a given threshold $C > 0$, e.g. for $C=1$:\n",
    "\n",
    "$$\n",
    "\\{ x_i, x_j \\} \\in E \\ \\Leftrightarrow \\|x_i - x_j\\|_2 < 1.\n",
    "$$\n",
    "\n",
    "Undirected graphs can be represented via an adjacency matrix, i.e. a matrix $A \\in \\mathbb{R}^{n\\times n}$, where $n$ is the number of data points. Here, $n = 350$. $A$ is given by\n",
    "\n",
    "$$\n",
    "A_{ij} =\n",
    "\\begin{cases}\n",
    "1 & \\text{if $\\{ x_i, x_j \\} \\in E \\ \\Leftrightarrow\\ \\|x_i - x_j\\|_2 < 1$}\\\\\n",
    "0 & \\text{otherwise.}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Clearly, since $G$ is undirected, $A$ is symmetric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Task 4:** Write a function to compute the adjacency matrix $A$ as defined above.\n",
    "\n",
    "Note: the entries of $A$ should be either $1$ or $0$, the diagonal entries $A_{ii}$ should be zero, and the result should be symmetric. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_graph( X ):\n",
    "    \"\"\"compute the adjacency matrix of the graph over the vertices X as given above\"\"\"\n",
    "    n = X.shape[0]\n",
    "    A = np.zeros([n,n])\n",
    "    \n",
    "    for p1 in range(n):\n",
    "        for p2 in range(p1+1,n):\n",
    "            if np.linalg.norm(X[p1]-X[p2]) < 1:\n",
    "                A[p1,p2] = 1\n",
    "                \n",
    "    A_t = np.transpose(A) #mirror A\n",
    "    A = A+A_t #Complete Matrix with mirrored side\n",
    "       \n",
    "    return A\n",
    "\n",
    "A = compute_graph(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the resulting graph using the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph( axes, X, A, color=None ):\n",
    "    \"\"\"create a plot of the graph with 2D vertices X and adjacency matrix A\"\"\"\n",
    "    \n",
    "    from matplotlib.collections import LineCollection\n",
    "\n",
    "    segs = []\n",
    "    for i, row in enumerate(A):\n",
    "        for j, a in enumerate(row):\n",
    "            if a > 0:\n",
    "                segs.append( [X[i,:], X[j,:]] )\n",
    "\n",
    "    lc = LineCollection(segs, linewidths=1.0, alpha=0.05, color='black')\n",
    "    \n",
    "    if color is None:\n",
    "        color = 'crimson'\n",
    "    \n",
    "    axes.scatter(X[:,0], X[:,1], 5.0, c=color, cmap='autumn', antialiased=True, zorder=5 )\n",
    "        \n",
    "    axes.add_artist(lc)\n",
    "    axes.axis('equal')\n",
    "    axes.grid()\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plot_graph(plt.gca(), X, A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there are many edges formed within the \"S\" part and the circle part, but few edges between them.\n",
    "\n",
    "The adjacency matrix $A$ can be inspected graphically using a so-called spy plot. This shows only the non-zero entries of $A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.spy( A )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the matrix is very sparsely populated, i.e. there are few non-zero entries, and these are scattered all over the matrix.\n",
    "\n",
    "The idea behind separating the two parts of the dataset is to now form a so-called graph cut. In essence, this means that we want to separate the data points into two disjoint classes $X = X_1 \\mathbin{\\unicode{x228D}} X_2$ and remove all edges between vertices in $X_1$ and $X_2$.\n",
    "\n",
    "While the assignment of each vertex to either $X_1$ or $X_2$ is in principle arbitrary, we can associate with each such cut a cost measure. If we choose $X_1$, then $X_2 = X \\setminus X_1$. \n",
    "\n",
    "Let now $E(X_1,X_2)$ denote the number of edges between vertices in $X_1$ and vertices in $X_2$, then the **normalized cut** for a particular choice of $X_1$ is given as:\n",
    "\n",
    "$$\n",
    "\\mathrm{ncut}_{X_1} = \\frac{|E(X_1,X_2)|}{|E(X_1,X)|} + \\frac{|E(X_1,X_2)|}{|E(X_2,X)|}\n",
    "$$\n",
    "\n",
    "The above formula measures how many edges are cut by the partition, relative to the overall number of edges among vertices of $X$, $X_1$, and $X_2$, respectively. An optimal separation, called a **minimal cut**, minimizes $\\mathrm{ncut}_{X_1}$ over all choices of $X_1$.\n",
    "\n",
    "While one could exhaustively try out all possible choices of $X_1$, there is an elegant connection to linear algebra.\n",
    "\n",
    "By a complex mathematical derivation that is not of too much interest here (but is given [here](https://en.wikipedia.org/wiki/Segmentation-based_object_categorization#Segmentation_using_normalized_cuts)) the minimal normalized cut, i.e. the best partition of $X$ that cuts the fewest edges, can be obtained from the eigenvector corresponding to the _smallest non-zero eigenvalue_ of the graph Laplacian $L$, given by \n",
    "\n",
    "$$\n",
    "L_{ij} = \\begin{cases}\n",
    "\\mathrm{deg}(v_i) & \\text{if $i = j$} \\\\\n",
    "-1 & \\text{if $\\{v_i, v_j\\} \\in E$} \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where $\\mathrm{deg}(v_i)$ is the degree of $v_i$, that is the number of edges connected to $v_i$. \n",
    "\n",
    "$L$ can also be computed more compactly as $L = D - A$, where $A$ is the adjacency matrix as above and $D$ is the diagonal degree matrix, $D_{ii} = \\mathrm{deg}(v_i)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Task 5:** Write a function to compute the graph Laplacian $L$ as given above, and visualize the result using a spy plot.\n",
    "\n",
    "Hint: The number of edges connecting to $v_i$ is the sum of entries in the $i$-th row or column of $A$.\n",
    "\n",
    "Note: the entries of $L$ should be either $0$, $-1$ or a positive integer, and $L$ should be symmetric. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_laplacian( A ):\n",
    "    \"\"\"compute the graph Laplacian matrix of the graph given by adjacency matrix A\"\"\"\n",
    "    #calculate D\n",
    "    n = A.shape[0]\n",
    "    D = np.zeros([n,n])\n",
    "    for i in range(n):\n",
    "        deg = sum(A[i])\n",
    "        D[i,i] = deg\n",
    "\n",
    "    return D-A\n",
    "\n",
    "\n",
    "L = graph_laplacian(A)\n",
    "plt.spy(L)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the spy plot, $L$ has the nearly the same pattern (called *sparsity structure*) as $A$; they only differ in the diagonal, where $A$ had zero entries.\n",
    "\n",
    "Let's next compute the smallest non-zero eigenvalue of $L$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Task 6**: Write a function to return the smallest non-zero eigenvalue of $L$, given the eigenvalues and corresponding eigenvectors in two arrays `vals` and `vecs`. \n",
    "\n",
    "Notes: \n",
    "- The input to your function is generated by [`numpy.linalg.eig`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.eig.html), which compute the eigenvalues and eigenvectors of $L$, in unsorted order.\n",
    "- Set the eigenvalues that are numerically close to zero ($< 10^{-9}$) to a very large number (e.g. $10^9$).\n",
    "- Sort the eigenvalues/eigenvectors from smallest to largest. Use [`numpy.argsort`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.argsort.html) function, which instead of sorting an array directly, returns an array of indices such that the input is in sorted order.\n",
    "- Return the eigenvector corresponding to the now left over smallest eigenvalue (i.e. the column of `vals` with the same index as the smallest eigenvalue).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smallest_nonzero_eigenvector(vals, vecs):\n",
    "    \"\"\"return the eigenvector to the smallest non-zero eigenvalue of the input L\"\"\"\n",
    "    \n",
    "    #set eigenvalues numerically close to zero to very large number\n",
    "    for i in range(vals.shape[0]):\n",
    "        if vals[i] < 10**(-9):\n",
    "            vals[i] = 10**9\n",
    "            \n",
    "    #sort the eigenvalues\n",
    "    vals_sorted = np.argsort(vals)\n",
    "    \n",
    "    #return the smallest eigenvalue\n",
    "    smallest_val = vals[vals_sorted[0]]\n",
    "    smallest_vec = vecs[:,vals_sorted[0]]\n",
    "    \n",
    "    return smallest_val,smallest_vec\n",
    "    \n",
    "\n",
    "vals, vecs = np.linalg.eig(L)\n",
    "val_min, vec_min = smallest_nonzero_eigenvector(vals, vecs)\n",
    "\n",
    "print(\"smallest non-zero eigenvalue:\", val_min)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything is correct, the smallest non-zero eigenvalue should be approximately $0.4$.\n",
    "\n",
    "The components of eigenvector can be interpreted as a function that assigns a real number to every vertex, and `vec_min[i]` is the function value for vertex $v_i$. Let's visualize this function; thankfully, `plot_graph` already supports this through an optional `color` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "plot_graph(plt.gca(), X, A, color=np.real(vec_min))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mission accomplished! This process is called **spectral clustering**. (\"Spectral\" indicates that this technique is related to eigenvalue analysis.) High function values ($> 0$, yellowish) correspond to the set $X_1$ – the \"S\" -- while low function values ($\\leq 0$, reddish) correspond to the other set $X_2$, the surrounding circle.\n",
    "\n",
    "If everything is correct, the image should look roughly like this (possibly with reversed colors):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "plot_graph(plt.gca(), X, A, color=(np.linalg.norm(X, axis=1) < 1.5)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Of course this trivial separation of points by radius would not work in the general case, but it works well for this test case.)\n",
    "\n",
    "One more thing: we can reorder the vertices of the graph such that the vertices in $X_1$ are before those of $X_2$. This does interesting things to the adjacency matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.argsort(vec_min > 0)\n",
    "plt.spy( A[idx].T[idx] );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, this reorders the adjacency matrix into two large blocks, with a minimal number of entries outside these blocks. The latter are exactly the edges that are cut by the minimal cut above."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
